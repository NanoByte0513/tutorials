# Encoder-only Family: Bert and its Variants
## BERT
Bi-directional Encoder Representation Transformer.
### Bidirectional Training
Mask a random word in a sentence, train the model to predict the masked word, the model can see the prior and posterior word.
### Ability Compare to GPT
Better in comprehensive ability, less better in generating ability.

## RoBERTa

## ALBERT


# Decoder-only Family: GPT
## GPT
Generative Pre-trained transformer. A **single-directional** model, when generating a word during training, only the prior words can be seen.